# Building an AI-Powered AIGP Exam Preparation System: Concepts and Implementation Guide

## Introduction

The AI Governance Professional (AIGP) certification has become essential for professionals working with artificial intelligence systems. This comprehensive guide explores how to build an intelligent exam preparation platform that combines modern web technologies with advanced AI capabilities to create contextual, relevant practice questions from study materials.

---

## Understanding AIGP Certification

### What is AIGP?

The **AI Governance Professional (AIGP)** certification validates expertise in managing AI systems responsibly. It covers four critical domains that every AI professional should master:

**1. AI Governance Foundations (25%)**
This domain establishes the fundamental understanding of AI governance principles:
- **AI System Types**: Understanding different categories of AI (narrow AI, general AI, machine learning systems)
- **Governance Frameworks**: Implementing organizational structures for AI oversight
- **Stakeholder Management**: Coordinating between technical teams, legal departments, and business units
- **Policy Development**: Creating comprehensive AI usage policies that balance innovation with responsibility

**2. AI Risk Management (30%)**
The largest domain focuses on identifying and mitigating AI-specific risks:
- **Bias Detection**: Recognizing algorithmic bias in training data, model design, and deployment
- **Safety Considerations**: Ensuring AI systems operate safely in critical applications like healthcare and transportation
- **Security Vulnerabilities**: Protecting against adversarial attacks, data poisoning, and model theft
- **Monitoring Systems**: Implementing continuous oversight of AI system performance and behavior

**3. Legal and Regulatory Landscape (25%)**
This domain addresses the evolving legal environment:
- **Current Regulations**: Understanding how GDPR, CCPA, and sector-specific laws apply to AI
- **Emerging Legislation**: Preparing for new AI-specific regulations like the EU AI Act
- **Compliance Frameworks**: Implementing processes to ensure ongoing regulatory compliance
- **International Standards**: Aligning with ISO/IEC standards and industry best practices

**4. AI Implementation and Operations (20%)**
The practical aspects of AI governance:
- **Program Implementation**: Establishing governance processes within existing organizational structures
- **Audit and Monitoring**: Creating systems for ongoing oversight and performance evaluation
- **Incident Response**: Developing procedures for handling AI-related failures or ethical breaches
- **Vendor Management**: Ensuring third-party AI services meet governance requirements

### Key Resources for AIGP Study
- [IAPP AIGP Certification](https://iapp.org/certify/aigp/) - Official certification details
- [EU AI Act](https://artificialintelligenceact.eu/) - Comprehensive AI regulation framework
- [NIST AI Risk Management Framework](https://www.nist.gov/itl/ai-risk-management-framework) - US government guidance
- [Partnership on AI](https://partnershiponai.org/) - Industry best practices and research

---

## Core System Concepts

### Document Processing Pipeline

Our system transforms PDF study materials into structured, searchable knowledge through several key stages:

**Text Extraction Concepts**
PDF processing involves more than simple text extraction. We need to handle:
- Multiple text encodings and fonts
- Complex layouts with columns and tables
- Image-based text requiring OCR
- Metadata extraction for context

```python
# Basic PDF text extraction concept
import fitz  # PyMuPDF

def extract_text_with_metadata(pdf_path):
    doc = fitz.open(pdf_path)
    
    extracted_content = {
        "metadata": doc.metadata,  # Author, title, creation date
        "pages": []
    }
    
    for page_num in range(len(doc)):
        page = doc[page_num]
        text = page.get_text()
        
        # Clean common PDF artifacts
        cleaned_text = text.replace('\n', ' ').strip()
        
        extracted_content["pages"].append({
            "page_number": page_num + 1,
            "text": cleaned_text,
            "word_count": len(cleaned_text.split())
        })
    
    return extracted_content
```

**Intelligent Chunking Strategies**
Rather than splitting text arbitrarily, we preserve semantic meaning:

```python
# Semantic-aware text chunking
def create_semantic_chunks(text, max_words=300):
    sentences = text.split('. ')
    chunks = []
    current_chunk = []
    current_word_count = 0
    
    for sentence in sentences:
        sentence_words = len(sentence.split())
        
        # Check if adding this sentence exceeds limit
        if current_word_count + sentence_words > max_words and current_chunk:
            # Save current chunk
            chunk_text = '. '.join(current_chunk) + '.'
            chunks.append({
                "text": chunk_text,
                "word_count": current_word_count,
                "sentence_count": len(current_chunk)
            })
            
            # Start new chunk
            current_chunk = [sentence]
            current_word_count = sentence_words
        else:
            current_chunk.append(sentence)
            current_word_count += sentence_words
    
    # Add final chunk
    if current_chunk:
        chunk_text = '. '.join(current_chunk) + '.'
        chunks.append({
            "text": chunk_text,
            "word_count": current_word_count,
            "sentence_count": len(current_chunk)
        })
    
    return chunks
```

**Vector Embeddings for Semantic Search**
Text embeddings convert human language into mathematical representations that computers can understand and compare:

```python
# Creating embeddings for semantic similarity
from sentence_transformers import SentenceTransformer

def generate_embeddings(text_chunks):
    # Load pre-trained model optimized for semantic similarity
    model = SentenceTransformer('all-MiniLM-L6-v2')
    
    # Extract just the text from chunks
    texts = [chunk["text"] for chunk in text_chunks]
    
    # Generate embeddings - converts text to numerical vectors
    embeddings = model.encode(texts)
    
    # Add embeddings to chunk data
    for i, chunk in enumerate(text_chunks):
        chunk["embedding"] = embeddings[i]
        chunk["embedding_model"] = "all-MiniLM-L6-v2"
    
    return text_chunks
```

---

## AI-Powered Question Generation

### Retrieval-Augmented Generation (RAG) Concepts

RAG combines the power of large language models with specific document knowledge:

**Semantic Search Process**
When a user requests questions on a topic, we:
1. Convert the topic into an embedding vector
2. Find the most similar document chunks
3. Use these chunks as context for question generation

```python
# Semantic search for relevant content
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

def find_relevant_chunks(query, document_chunks, top_k=5):
    # Generate embedding for the search query
    model = SentenceTransformer('all-MiniLM-L6-v2')
    query_embedding = model.encode([query])
    
    # Get embeddings from all chunks
    chunk_embeddings = np.array([chunk["embedding"] for chunk in document_chunks])
    
    # Calculate similarity scores
    similarities = cosine_similarity(query_embedding, chunk_embeddings)[0]
    
    # Get indices of most similar chunks
    top_indices = np.argsort(similarities)[::-1][:top_k]
    
    # Return relevant chunks with similarity scores
    relevant_chunks = []
    for idx in top_indices:
        chunk = document_chunks[idx].copy()
        chunk["similarity_score"] = similarities[idx]
        relevant_chunks.append(chunk)
    
    return relevant_chunks
```

**Advanced Prompt Engineering for AIGP**
The quality of generated questions depends heavily on well-crafted prompts:

```python
# AIGP-specific question generation
def create_aigp_question_prompt(context_chunks, difficulty="intermediate"):
    # Combine relevant chunks into context
    context = "\n\n".join([chunk["text"] for chunk in context_chunks])
    
    difficulty_instructions = {
        "beginner": "Focus on basic definitions and fundamental concepts",
        "intermediate": "Require analysis and practical application",
        "advanced": "Present complex scenarios requiring strategic thinking"
    }
    
    prompt = f"""
You are an expert in AI Governance preparing questions for the AIGP certification.

CONTEXT FROM STUDY MATERIALS:
{context}

INSTRUCTIONS:
- Create a multiple-choice question with 4 options (A, B, C, D)
- {difficulty_instructions[difficulty]}
- Ensure the question tests understanding of AI governance principles
- Include realistic scenarios that professionals might encounter
- Provide a detailed explanation for the correct answer

AIGP DOMAIN FOCUS:
Consider these key areas: governance frameworks, risk management, 
regulatory compliance, ethical considerations, and implementation practices.

Generate one high-quality question:
"""
    
    return prompt
```

**OpenAI Integration with Error Handling**
Robust AI integration requires proper error handling and retry logic:

```python
# Robust OpenAI integration
import openai
import time
from typing import Optional

def generate_question_with_openai(prompt: str, max_retries=3) -> Optional[dict]:
    client = openai.OpenAI(api_key="your-api-key")
    
    for attempt in range(max_retries):
        try:
            response = client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "You are an AIGP certification expert."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,  # Balanced creativity and consistency
                max_tokens=1000
            )
            
            # Parse the response
            question_text = response.choices[0].message.content
            
            # Basic validation
            if len(question_text) < 100:
                raise ValueError("Generated question too short")
            
            return {
                "question": question_text,
                "model_used": "gpt-4",
                "generation_time": time.time(),
                "attempt": attempt + 1
            }
            
        except openai.RateLimitError:
            # Wait before retrying for rate limits
            wait_time = (2 ** attempt) * 60  # Exponential backoff
            print(f"Rate limit hit, waiting {wait_time} seconds...")
            time.sleep(wait_time)
            
        except Exception as e:
            print(f"Attempt {attempt + 1} failed: {str(e)}")
            if attempt == max_retries - 1:
                return None
    
    return None
```

---

## Frontend Development Concepts

### Modern React with Astro Framework

**Component-Based Architecture**
Astro's island architecture allows us to build fast, interactive interfaces:

```tsx
// DocumentUpload.tsx - Smart file upload component
import React, { useState, useCallback } from 'react';

interface DocumentUploadProps {
  onUploadSuccess: (document: any) => void;
  onUploadError: (error: string) => void;
}

export const DocumentUpload: React.FC<DocumentUploadProps> = ({
  onUploadSuccess,
  onUploadError
}) => {
  const [isUploading, setIsUploading] = useState(false);
  const [uploadProgress, setUploadProgress] = useState(0);

  const handleFileUpload = useCallback(async (file: File) => {
    // Validate file type and size
    if (!file.name.endsWith('.pdf')) {
      onUploadError('Please upload a PDF file');
      return;
    }

    if (file.size > 50 * 1024 * 1024) { // 50MB limit
      onUploadError('File size must be less than 50MB');
      return;
    }

    setIsUploading(true);
    setUploadProgress(0);

    try {
      const formData = new FormData();
      formData.append('file', file);

      // Simulate upload progress
      const progressInterval = setInterval(() => {
        setUploadProgress(prev => Math.min(prev + 10, 90));
      }, 200);

      const response = await fetch('/api/upload', {
        method: 'POST',
        body: formData
      });

      clearInterval(progressInterval);
      setUploadProgress(100);

      if (!response.ok) {
        throw new Error('Upload failed');
      }

      const result = await response.json();
      onUploadSuccess(result);

    } catch (error) {
      onUploadError(error.message || 'Upload failed');
    } finally {
      setIsUploading(false);
      setUploadProgress(0);
    }
  }, [onUploadSuccess, onUploadError]);

  return (
    <div className="upload-container">
      {isUploading ? (
        <div className="upload-progress">
          <div className="progress-bar" style={{width: `${uploadProgress}%`}} />
          <span>Uploading... {uploadProgress}%</span>
        </div>
      ) : (
        <input
          type="file"
          accept=".pdf"
          onChange={(e) => {
            const file = e.target.files?.[0];
            if (file) handleFileUpload(file);
          }}
        />
      )}
    </div>
  );
};
```

**State Management with Custom Hooks**
Modern React state management for complex applications:

```tsx
// useQuestionGeneration.ts - Custom hook for question management
import { useState, useCallback } from 'react';

interface GenerationSettings {
  documentId: string;
  numQuestions: number;
  difficulty: 'beginner' | 'intermediate' | 'advanced';
  topicFocus?: string;
}

export const useQuestionGeneration = () => {
  const [questions, setQuestions] = useState([]);
  const [isGenerating, setIsGenerating] = useState(false);
  const [generationError, setGenerationError] = useState<string | null>(null);

  const generateQuestions = useCallback(async (settings: GenerationSettings) => {
    setIsGenerating(true);
    setGenerationError(null);

    try {
      const response = await fetch('/api/generate-questions', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(settings)
      });

      if (!response.ok) {
        throw new Error('Question generation failed');
      }

      const data = await response.json();
      setQuestions(data.questions);

      return data.questions;

    } catch (error) {
      setGenerationError(error.message);
      throw error;
    } finally {
      setIsGenerating(false);
    }
  }, []);

  const clearQuestions = useCallback(() => {
    setQuestions([]);
    setGenerationError(null);
  }, []);

  return {
    questions,
    isGenerating,
    generationError,
    generateQuestions,
    clearQuestions
  };
};
```

---

## Backend API Development

### FastAPI for High-Performance APIs

**Structured API Design**
FastAPI provides excellent developer experience with automatic documentation:

```python
# main.py - Core API structure
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional
import logging

app = FastAPI(
    title="AIGP Exam Prep API",
    description="AI-powered question generation for AIGP certification",
    version="1.0.0"
)

# Enable CORS for frontend communication
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://localhost:4321"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"]
)

# Data models for type safety
class QuestionRequest(BaseModel):
    document_name: str
    num_questions: int = 5
    difficulty: str = "intermediate"
    topic_focus: Optional[str] = None

class GeneratedQuestion(BaseModel):
    question: str
    options: List[str]
    correct_answer: str
    explanation: str
    difficulty: str

@app.post("/upload")
async def upload_document(file: UploadFile = File(...)):
    """Upload and process PDF documents"""
    
    # Validate file type
    if not file.filename.endswith('.pdf'):
        raise HTTPException(status_code=400, detail="Only PDF files supported")
    
    try:
        # Save uploaded file
        file_path = f"uploads/{file.filename}"
        with open(file_path, "wb") as buffer:
            content = await file.read()
            buffer.write(content)
        
        # Process document (extract text, create chunks, generate embeddings)
        result = await process_document(file.filename, file_path)
        
        return {
            "filename": file.filename,
            "status": "processed",
            "chunks_created": result["chunk_count"],
            "processing_time": result["processing_time"]
        }
        
    except Exception as e:
        logging.error(f"Upload error: {str(e)}")
        raise HTTPException(status_code=500, detail="Document processing failed")

@app.post("/generate-questions", response_model=List[GeneratedQuestion])
async def generate_questions(request: QuestionRequest):
    """Generate practice questions from processed documents"""
    
    try:
        # Find relevant document chunks
        relevant_chunks = await find_document_chunks(
            request.document_name, 
            request.topic_focus
        )
        
        if not relevant_chunks:
            raise HTTPException(
                status_code=404, 
                detail="No relevant content found"
            )
        
        # Generate questions using AI
        questions = await create_questions(
            chunks=relevant_chunks,
            num_questions=request.num_questions,
            difficulty=request.difficulty
        )
        
        return questions
        
    except Exception as e:
        logging.error(f"Question generation error: {str(e)}")
        raise HTTPException(
            status_code=500, 
            detail="Question generation failed"
        )
```

**Asynchronous Processing for Better Performance**
Handle long-running operations without blocking the API:

```python
# async_processor.py - Background task processing
import asyncio
from typing import Dict, Any

async def process_document_async(filename: str, file_path: str) -> Dict[str, Any]:
    """Process document asynchronously to avoid blocking API"""
    
    # Step 1: Extract text (I/O bound operation)
    text_content = await asyncio.to_thread(extract_pdf_text, file_path)
    
    # Step 2: Create chunks (CPU bound operation)
    chunks = await asyncio.to_thread(create_text_chunks, text_content)
    
    # Step 3: Generate embeddings (I/O bound - API calls)
    embeddings = await generate_embeddings_async(chunks)
    
    # Step 4: Store in vector database
    await store_embeddings_async(filename, chunks, embeddings)
    
    return {
        "chunk_count": len(chunks),
        "total_words": sum(chunk["word_count"] for chunk in chunks),
        "processing_time": time.time() - start_time
    }

async def generate_embeddings_async(chunks: List[Dict]) -> List:
    """Generate embeddings asynchronously for better performance"""
    
    # Process chunks in batches to avoid overwhelming the embedding service
    batch_size = 10
    all_embeddings = []
    
    for i in range(0, len(chunks), batch_size):
        batch = chunks[i:i + batch_size]
        batch_texts = [chunk["text"] for chunk in batch]
        
        # Use async HTTP client for embedding API calls
        batch_embeddings = await call_embedding_api(batch_texts)
        all_embeddings.extend(batch_embeddings)
        
        # Small delay to respect API rate limits
        await asyncio.sleep(0.1)
    
    return all_embeddings
```

---

## Advanced AI Integration Concepts

### Quality Control and Validation

**Multi-Layer Question Validation**
Ensure generated questions meet AIGP standards:

```python
# question_validator.py - Quality assurance for generated content
import re
from typing import Dict, List, Tuple

class AIGPQuestionValidator:
    def __init__(self):
        self.aigp_keywords = {
            'governance': ['governance', 'oversight', 'accountability', 'framework'],
            'risk': ['risk', 'bias', 'fairness', 'safety', 'security'],
            'legal': ['regulation', 'compliance', 'legal', 'privacy', 'GDPR'],
            'ethics': ['ethics', 'transparency', 'explainability', 'responsibility']
        }
    
    def validate_question(self, question_data: Dict) -> Tuple[bool, List[str]]:
        """Comprehensive validation of generated questions"""
        
        issues = []
        
        # Check required components
        required_fields = ['question', 'options', 'correct_answer', 'explanation']
        for field in required_fields:
            if not question_data.get(field):
                issues.append(f"Missing {field}")
        
        # Validate question structure
        if len(question_data.get('options', [])) != 4:
            issues.append("Must have exactly 4 answer options")
        
        # Check question clarity
        question_text = question_data.get('question', '')
        if len(question_text.split()) < 10:
            issues.append("Question too short - lacks detail")
        
        if '?' not in question_text:
            issues.append("Question must end with question mark")
        
        # Validate AIGP relevance
        full_text = ' '.join([
            question_data.get('question', ''),
            ' '.join(question_data.get('options', [])),
            question_data.get('explanation', '')
        ]).lower()
        
        keyword_found = False
        for category, keywords in self.aigp_keywords.items():
            if any(keyword in full_text for keyword in keywords):
                keyword_found = True
                break
        
        if not keyword_found:
            issues.append("Question lacks AIGP-relevant terminology")
        
        # Check explanation quality
        explanation = question_data.get('explanation', '')
        if len(explanation.split()) < 20:
            issues.append("Explanation too brief - should be more detailed")
        
        return len(issues) == 0, issues
    
    def suggest_improvements(self, question_data: Dict, issues: List[str]) -> List[str]:
        """Provide specific suggestions for improving questions"""
        
        suggestions = []
        
        if "Question too short" in issues:
            suggestions.append("Add more context or scenario details to the question")
        
        if "lacks AIGP-relevant terminology" in issues:
            suggestions.append("Include specific AI governance concepts like 'algorithmic accountability' or 'bias mitigation'")
        
        if "Explanation too brief" in issues:
            suggestions.append("Expand explanation to include why other options are incorrect")
        
        return suggestions
```

**Adaptive Difficulty Adjustment**
Dynamically adjust question difficulty based on user performance:

```python
# difficulty_manager.py - Adaptive learning system
class DifficultyManager:
    def __init__(self):
        self.user_performance = {}
    
    def track_performance(self, user_id: str, question_id: str, 
                         correct: bool, time_taken: int):
        """Track user performance for adaptive difficulty"""
        
        if user_id not in self.user_performance:
            self.user_performance[user_id] = {
                'total_questions': 0,
                'correct_answers': 0,
                'average_time': 0,
                'recent_performance': []
            }
        
        user_stats = self.user_performance[user_id]
        user_stats['total_questions'] += 1
        
        if correct:
            user_stats['correct_answers'] += 1
        
        # Track recent performance (last 10 questions)
        user_stats['recent_performance'].append({
            'correct': correct,
            'time_taken': time_taken
        })
        
        if len(user_stats['recent_performance']) > 10:
            user_stats['recent_performance'].pop(0)
    
    def recommend_difficulty(self, user_id: str) -> str:
        """Recommend difficulty level based on performance"""
        
        if user_id not in self.user_performance:
            return 'intermediate'  # Default starting level
        
        stats = self.user_performance[user_id]
        recent_performance = stats['recent_performance']
        
        if len(recent_performance) < 3:
            return 'intermediate'
        
        # Calculate recent accuracy
        recent_correct = sum(1 for q in recent_performance if q['correct'])
        accuracy = recent_correct / len(recent_performance)
        
        # Calculate average time (lower is better)
        avg_time = sum(q['time_taken'] for q in recent_performance) / len(recent_performance)
        
        # Adjust difficulty based on performance
        if accuracy >= 0.8 and avg_time < 45:  # High accuracy, fast responses
            return 'advanced'
        elif accuracy >= 0.6 and avg_time < 60:  # Good accuracy, reasonable time
            return 'intermediate'
        else:  # Lower accuracy or slow responses
            return 'beginner'
```

---

## Performance Optimization Strategies

### Caching and Memory Management

**Intelligent Caching System**
Optimize performance by caching frequently used data:

```python
# cache_manager.py - Smart caching for AI operations
import hashlib
import pickle
import time
from typing import Any, Optional

class SmartCache:
    def __init__(self, max_size_mb: int = 100):
        self.cache = {}
        self.access_times = {}
        self.max_size_bytes = max_size_mb * 1024 * 1024
        self.current_size = 0
    
    def get_cache_key(self, data: Any) -> str:
        """Generate unique cache key for any data"""
        data_str = str(data)
        return hashlib.md5(data_str.encode()).hexdigest()
    
    def get(self, key: str) -> Optional[Any]:
        """Retrieve item from cache"""
        if key in self.cache:
            self.access_times[key] = time.time()
            return self.cache[key]
        return None
    
    def set(self, key: str, value: Any, ttl: int = 3600):
        """Store item in cache with time-to-live"""
        
        # Calculate size of new item
        item_size = len(pickle.dumps(value))
        
        # Make room if necessary
        while self.current_size + item_size > self.max_size_bytes:
            self._evict_oldest()
        
        # Store item
        self.cache[key] = {
            'value': value,
            'created_at': time.time(),
            'ttl': ttl,
            'size': item_size
        }
        self.access_times[key] = time.time()
        self.current_size += item_size
    
    def _evict_oldest(self):
        """Remove least recently used item"""
        if not self.cache:
            return
        
        oldest_key = min(self.access_times.keys(), 
                        key=lambda k: self.access_times[k])
        
        item = self.cache.pop(oldest_key)
        self.access_times.pop(oldest_key)
        self.current_size -= item['size']

# Usage example for embedding cache
embedding_cache = SmartCache(max_size_mb=200)

def get_cached_embedding(text: str):
    """Get embedding from cache or generate new one"""
    cache_key = embedding_cache.get_cache_key(text)
    
    # Try to get from cache first
    cached_result = embedding_cache.get(cache_key)
    if cached_result:
        return cached_result['value']
    
    # Generate new embedding
    model = SentenceTransformer('all-MiniLM-L6-v2')
    embedding = model.encode([text])[0]
    
    # Cache for future use
    embedding_cache.set(cache_key, embedding, ttl=7200)  # 2 hours
    
    return embedding
```

### Database Optimization

**Efficient Vector Storage with FAISS**
Optimize vector similarity search for large document collections:

```python
# vector_store.py - Optimized vector storage and retrieval
import faiss
import numpy as np
from typing import List, Dict, Tuple

class OptimizedVectorStore:
    def __init__(self, dimension: int = 384):
        self.dimension = dimension
        self.index = None
        self.metadata = []
        
    def build_index(self, embeddings: np.ndarray, metadata: List[Dict]):
        """Build optimized FAISS index based on data size"""
        
        n_vectors = embeddings.shape[0]
        
        # Choose index type based on collection size
        if n_vectors < 1000:
            # Small collection: use exact search
            self.index = faiss.IndexFlatIP(self.dimension)
            
        elif n_vectors < 50000:
            # Medium collection: use IVF (Inverted File Index)
            nlist = min(int(np.sqrt(n_vectors)), 1000)
            quantizer = faiss.IndexFlatIP(self.dimension)
            self.index = faiss.IndexIVFFlat(quantizer, self.dimension, nlist)
            
            # Train the index
            self.index.train(embeddings.astype('float32'))
            
        else:
            # Large collection: use more sophisticated compression
            nlist = min(int(np.sqrt(n_vectors)), 4000)
            m = 8  # Number of subquantizers
            quantizer = faiss.IndexFlatIP(self.dimension)
            self.index = faiss.IndexIVFPQ(quantizer, self.dimension, nlist, m, 8)
            
            # Train the index
            self.index.train(embeddings.astype('float32'))
        
        # Normalize embeddings for cosine similarity
        faiss.normalize_L2(embeddings.astype('float32'))
        
        # Add vectors to index
        self.index.add(embeddings.astype('float32'))
        
        # Store metadata
        self.metadata = metadata
    
    def search(self, query_embedding: np.ndarray, 
               k: int = 5, threshold: float = 0.5) -> List[Dict]:
        """Perform optimized similarity search"""
        
        # Normalize query
        query_norm = query_embedding.reshape(1, -1).astype('float32')
        faiss.normalize_L2(query_norm)
        
        # Search
        scores, indices = self.index.search(query_norm, k * 2)
        
        # Filter and format results
        results = []
        for score, idx in zip(scores[0], indices[0]):
            if score >= threshold and idx < len(self.metadata):
                results.append({
                    'metadata': self.metadata[idx],
                    'similarity_score': float(score),
                    'chunk_id': idx
                })
        
        return results[:k]
```

---

## Testing and Quality Assurance

### Comprehensive Testing Strategy

**Unit Testing for AI Components**
Test individual components to ensure reliability:

```python
# test_question_generation.py - Unit tests for AI components
import unittest
from unittest.mock import Mock, patch
import numpy as np

class TestQuestionGeneration(unittest.TestCase):
    
    def setUp(self):
        self.sample_chunks = [
            {
                "text": "AI governance involves establishing frameworks for responsible AI development and deployment.",
                "embedding": np.random.rand(384),
                "topics": ["governance"]
            },
            {
                "text": "Bias in AI systems can arise from training data, algorithmic design, or deployment context.",
                "embedding": np.random.rand(384),
                "topics": ["risk_management", "bias"]
            }
        ]
    
    def test_chunk_relevance_scoring(self):
        """Test semantic similarity scoring"""
        from your_module import calculate_relevance
        
        query = "AI governance frameworks"
        scores = calculate_relevance(query, self.sample_chunks)
        
        # Should return scores for all chunks
        self.assertEqual(len(scores), len(self.sample_chunks))
        
        # Scores should be between 0 and 1
        for score in scores:
            self.assertGreaterEqual(score, 0)
            self.assertLessEqual(score, 1)
        
        # First chunk should be more relevant to governance query
        self.assertGreater(scores[0], scores[1])
    
    @patch('openai.ChatCompletion.create')
    def test_question_generation_with_mock(self, mock_openai):
        """Test question generation with mocked OpenAI response"""
        
        # Mock OpenAI response
        mock_response = Mock()
        mock_response.choices = [Mock()]
        mock_response.choices[0].message.content = """
        Question: What is the primary purpose of AI governance frameworks?
        A) To slow down AI development
        B) To ensure responsible AI development and deployment
        C) To eliminate all AI risks
        D) To replace human decision-making
        
        Correct Answer: B
        
        Explanation: AI governance frameworks are designed to ensure that AI systems are developed and deployed responsibly, balancing innovation with risk management and ethical considerations.
        """
        mock_openai.return_value = mock_response
        
        # Test question generation
        from your_module import generate_question
        
        result = generate_question(self.sample_chunks[0], difficulty="intermediate")
        
        # Verify OpenAI was called
        mock_openai.assert_called_once()
        
        # Verify result structure
        self.assertIn('question', result)
        self.assertIn('options', result)
        self.assertIn('correct_answer', result)
        self.assertIn('explanation', result)
    
    def test_question_validation(self):
        """Test question quality validation"""
        from your_module import validate_question
        
        valid_question = {
            'question': 'What is the primary goal of AI bias mitigation strategies?',
            'options': ['A) Option 1', 'B) Option 2', 'C) Option 3', 'D) Option 4'],
            'correct_answer': 'B',
            'explanation': 'This is a detailed explanation of why option B is correct and why the other options are incorrect.'
        }
        
        is_valid, issues = validate_question(valid_question)
        
        self.assertTrue(is_valid)
        self.assertEqual(len(issues), 0)
        
        # Test invalid question
        invalid_question = {
            'question': 'What?',  # Too short
            'options': ['A', 'B'],  # Wrong number of options
            'correct_answer': 'A',
            'explanation': 'Short.'  # Too brief
        }
        
        is_valid, issues = validate_question(invalid_question)
        
        self.assertFalse(is_valid)
        self.assertGreater(len(issues), 0)

if __name__ == '__main__':
    unittest.main()
```

**Integration Testing for API Endpoints**
Test complete workflows to ensure system reliability:

```python
# test_api_integration.py - Integration tests
import pytest
import asyncio
from fastapi.testclient import TestClient
from your_app import app

client = TestClient(app)

class TestAPIIntegration:
    
    def test_document_upload_workflow(self):
        """Test complete document upload and processing workflow"""
        
        # Create test PDF file
        test_file_content = b"Mock PDF content for testing"
        
        # Test upload
        response = client.post(
            "/upload",
            files={"file": ("test_document.pdf", test_file_content, "application/pdf")}
        )
        
        assert response.status_code == 200
        upload_result = response.json()
        
        assert "filename" in upload_result
        assert "status" in upload_result
        assert upload_result["status"] == "processed"
    
    def test_question_generation_workflow(self):
        """Test question generation workflow"""
        
        # First upload a document (assume it exists)
        document_name = "test_document"
        
        # Test question generation
        generation_request = {
            "document_name": document_name,
            "num_questions": 3,
            "difficulty": "intermediate",
            "topic_focus": "AI governance"
        }
        
        response = client.post("/generate-questions", json=generation_request)
        
        if response.status_code == 200:
            questions = response.json()
            
            assert len(questions) <= 3  # Should not exceed requested number
            
            for question in questions:
                assert "question" in question
                assert "options" in question
                assert "correct_answer" in question
                assert "explanation" in question
                assert len(question["options"]) == 4
    
    def test_error_handling(self):
        """Test API error handling"""
        
        # Test invalid file upload
        response = client.post(
            "/upload",
            files={"file": ("test.txt", b"Not a PDF", "text/plain")}
        )
        
        assert response.status_code == 400
        
        # Test question generation for non-existent document
        response = client.post(
            "/generate-questions",
            json={"document_name": "nonexistent_document"}
        )
        
        assert response.status_code == 404
```

---

## Security and Privacy Considerations

### Data Protection Strategies

**Secure File Handling**
Implement robust security measures for document processing:

```python
# security_utils.py - Security utilities for file handling
import hashlib
import os
import tempfile
from pathlib import Path
from typing import Optional

class SecureFileHandler:
    def __init__(self, upload_dir: str = "uploads", max_file_size: int = 50 * 1024 * 1024):
        self.upload_dir = Path(upload_dir)
        self.upload_dir.mkdir(exist_ok=True)
        self.max_file_size = max_file_size
        self.allowed_extensions = {'.pdf'}
    
    def validate_file(self, filename: str, content: bytes) -> tuple[bool, Optional[str]]:
        """Validate uploaded file for security"""
        
        # Check file size
        if len(content) > self.max_file_size:
            return False, "File too large"
        
        # Check file extension
        file_path = Path(filename)
        if file_path.suffix.lower() not in self.allowed_extensions:
            return False, "Invalid file type"
        
        # Check for suspicious filename patterns
        if '..' in filename or '/' in filename or '\\' in filename:
            return False, "Invalid filename"
        
        # Basic content validation for PDF
        if not content.startswith(b'%PDF-'):
            return False, "Invalid PDF format"
        
        return True, None
    
    def secure_save(self, filename: str, content: bytes) -> str:
        """Securely save uploaded file"""
        
        # Generate unique filename to prevent conflicts
        file_hash = hashlib.md5(content).hexdigest()[:8]
        safe_filename = f"{file_hash}_{filename}"
        
        # Create secure file path
        file_path = self.upload_dir / safe_filename
        
        # Save with restricted permissions
        with open(file_path, 'wb') as f:
            f.write(content)
        
        # Set restrictive file permissions (owner read/write only)
        os.chmod(file_path, 0o600)
        
        return str(file_path)
    
    def secure_delete(self, file_path: str) -> bool:
        """Securely delete file"""
        try:
            path = Path(file_path)
            if path.exists() and path.parent == self.upload_dir:
                # Overwrite file content before deletion
                with open(path, 'wb') as f:
                    f.write(os.urandom(path.stat().st_size))
                path.unlink()
                return True
        except Exception:
            pass
        return False
```

**Privacy-Preserving AI Processing**
Minimize data exposure while maintaining functionality:

```python
# privacy_manager.py - Privacy-preserving processing
import hashlib
from typing import Dict, List, Any

class PrivacyManager:
    def __init__(self):
        self.sensitive_patterns = [
            r'\b\d{3}-\d{2}-\d{4}\b',  # SSN pattern
            r'\b\d{4}[\s-]?\d{4}[\s-]?\d{4}[\s-]?\d{4}\b',  # Credit card pattern
            r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'  # Email pattern
        ]
    
    def anonymize_text(self, text: str) -> str:
        """Remove or replace sensitive information"""
        import re
        
        anonymized = text
        
        # Replace sensitive patterns
        for pattern in self.sensitive_patterns:
            anonymized = re.sub(pattern, '[REDACTED]', anonymized)
        
        return anonymized
    
    def create_privacy_hash(self, text: str) -> str:
        """Create hash for privacy-preserving identification"""
        # Use SHA-256 for one-way hashing
        return hashlib.sha256(text.encode()).hexdigest()
    
    def minimal_metadata(self, original_metadata: Dict[str, Any]) -> Dict[str, Any]:
        """Extract only necessary metadata"""
        
        # Only keep non-sensitive metadata
        safe_fields = ['title', 'page_count', 'creation_date', 'file_size']
        
        minimal = {}
        for field in safe_fields:
            if field in original_metadata:
                value = original_metadata[field]
                # Anonymize title if it might contain sensitive info
                if field == 'title':
                    value = self.anonymize_text(str(value))
                minimal[field] = value
        
        return minimal
```

---
